{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6488d80e",
   "metadata": {},
   "source": [
    "# LawStory AI — Judgment PDF to Multi-Scene Explainer Video (Text-Based Prototype)\n",
    "\n",
    "This notebook is the **main source of truth** for evaluating the project.  \n",
    "It documents the end-to-end AI + Automation pipeline that converts a **court judgment PDF** into a **multi-scene explainer video** (white text on black background), without manual editing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports (no external API keys required for this notebook)\n",
    "import json\n",
    "print(\"Notebook initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060d65f",
   "metadata": {},
   "source": [
    "1) Problem Definition & Objective  \n",
    "a. Selected project track  \n",
    "\n",
    "AI + Automation (LLM + Video Rendering Pipeline)  \n",
    "This project is built under the **AI + Automation** track, combining **LLM-based legal understanding** with an automated **video rendering pipeline**. The goal is to build a system that can take a **court judgment PDF as the only input**, extract the most meaningful legal information, and automatically convert it into a short, structured **multi-scene explainer video**. The entire process is designed to work without manual editing, manual scriptwriting, or manual video creation.\n",
    "\n",
    "b. Clear problem statement  \n",
    "\n",
    "Legal judgments are long, technical, and written in a format that is difficult to consume quickly. Even when someone has access to the judgment PDF, understanding it requires significant time and effort to identify the key parts like **facts, legal issues, arguments from both sides, and the final reasoning of the court**. The problem this project solves is: **how to automatically convert a judgment PDF into an easy-to-consume, structured multi-scene video without manual intervention.** Without this kind of system, users are forced to either spend hours reading, depend on scattered online summaries, or miss the core learning from the judgment entirely.\n",
    "\n",
    "c. Real-world relevance and motivation  \n",
    "\n",
    "This problem is highly real and urgent because legal awareness and legal education depend on access to understandable information, but judgments are not written for quick comprehension.  \n",
    "- **Law students and common people** struggle to understand judgments quickly because of complex language, length, and legal formatting.  \n",
    "- **Lawyers, educators, and legal creators** need short explainers to teach, spread awareness, and simplify complex case law.  \n",
    "- Existing options usually require manual summarization or manual video editing, which is slow and not scalable.  \n",
    "- This pipeline makes legal content dramatically more accessible by converting it into an explainer format that can be consumed in minutes.  \n",
    "\n",
    "This project solves a gap that is not addressed effectively by traditional reading, short notes, or generic summaries. It can directly power **LawStory AI** as a product feature where a user uploads a judgment PDF and receives a structured explainer video output automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f0835d",
   "metadata": {},
   "source": [
    "2) Data Understanding & Preparation  \n",
    "a. Dataset source (public / synthetic / collected / API)  \n",
    "\n",
    "This project does not use a fixed “traditional dataset” like CSVs or labeled training data. Instead, it uses **live real-world input** in the form of **judgment PDFs**. The data source includes:  \n",
    "- **Collected / user-provided judgment PDF documents**  \n",
    "- **Extracted text generated from those PDFs** through processing inside the Make.com scenario  \n",
    "\n",
    "So the “data” here is **unstructured legal text extracted from actual court judgments**, making it realistic, variable, and challenging in a way that reflects real product usage.\n",
    "\n",
    "b. Data loading and exploration  \n",
    "\n",
    "The data enters the system through Make.com as:  \n",
    "- A **Webhook input** containing the judgment PDF URL/file  \n",
    "- An **HTTP module** that fetches/downloads the judgment PDF  \n",
    "- A **Custom JS module** that extracts PDF → text  \n",
    "- The extracted text is passed forward into the AI step  \n",
    "\n",
    "This stage functions as the exploration and validation stage where we confirm:  \n",
    "- The PDF is successfully downloaded and readable  \n",
    "- The extracted text is complete enough for understanding  \n",
    "- The text contains essential legal information such as court details and case context  \n",
    "- The text includes the core components required for video explanation such as:  \n",
    "  - **facts**  \n",
    "  - **legal issues**  \n",
    "  - **arguments from both sides**  \n",
    "  - **decision and reasoning / ratio**  \n",
    "\n",
    "c. Cleaning, preprocessing, feature engineering  \n",
    "\n",
    "Instead of numeric feature engineering, this project uses **LLM-ready preprocessing** to ensure the extracted legal text becomes usable for structured generation. The key preparation work included:  \n",
    "- Ensuring the extracted judgment text is passed as a clean, single input to Gemini  \n",
    "- Structuring the output into a predictable JSON format required for automation  \n",
    "- Converting raw legal content into a multi-scene video script that contains:  \n",
    "  - narration  \n",
    "  - duration_seconds  \n",
    "  - visual instructions  \n",
    "\n",
    "The most important transformation is:  \n",
    "**Raw judgment PDF text → structured multi-scene JSON video script**\n",
    "\n",
    "d. Handling missing values or noise (if applicable)  \n",
    "\n",
    "Yes, legal PDFs often contain noise such as:  \n",
    "- line breaks and spacing issues  \n",
    "- headers/footers repeated on every page  \n",
    "- inconsistent formatting across courts  \n",
    "- missing or unclear headings  \n",
    "\n",
    "This was handled practically by:  \n",
    "- relying on Gemini to generate clean structured JSON output instead of depending on raw formatting  \n",
    "- ensuring JSON is valid and parseable in Make  \n",
    "- designing prompts that still produce a complete explainer even if some parts are not explicitly labeled in the judgment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e73f8",
   "metadata": {},
   "source": [
    "3) Model / System Design  \n",
    "a. AI technique used (ML / DL / NLP / LLM / Recommendation / Hybrid)  \n",
    "\n",
    "This project is an **LLM-based NLP pipeline** combined with a video rendering system. It uses:  \n",
    "- **Google Gemini** for script generation in a structured multi-scene format  \n",
    "- **Shotstack API** for rendering the final video from those scenes  \n",
    "\n",
    "Therefore, it is best described as a **Hybrid AI + Media Rendering Automation system**, where the AI generates structured understanding and the renderer converts it into an output video.\n",
    "\n",
    "b. Architecture or pipeline explanation  \n",
    "\n",
    "Final working pipeline (multi-scene, one final video URL, no audio):  \n",
    "\n",
    "Webhook Trigger  \n",
    "- Receives the judgment PDF URL/request from Bubble  \n",
    "\n",
    "HTTP Module  \n",
    "- Downloads/fetches the judgment PDF  \n",
    "\n",
    "Custom JS  \n",
    "- Converts PDF → extracted text  \n",
    "\n",
    "Gemini Module  \n",
    "- Converts extracted judgment text into structured JSON video script  \n",
    "- Output includes:  \n",
    "  - title_frame metadata (court name, case title, year, citation, coram)  \n",
    "  - scenes[] array  \n",
    "\n",
    "Parse JSON module  \n",
    "- Reads Gemini output as JSON so Make can access scene fields like:  \n",
    "  - scenes[1].narration  \n",
    "  - scenes[1].duration_seconds  \n",
    "  - scenes[1].visual  \n",
    "\n",
    "Iterator  \n",
    "- Iterates over scenes[]  \n",
    "- Creates 1 bundle per scene  \n",
    "\n",
    "Text Aggregator (critical step)  \n",
    "- Combines all iterated scenes into one “clips array” for Shotstack  \n",
    "- Produces multiple clip objects such as:  \n",
    "  - text clip 1 (start=0)  \n",
    "  - text clip 2 (start=5)  \n",
    "  - text clip 3 (start=15)  \n",
    "  - and so on  \n",
    "\n",
    "Shotstack HTTP POST  \n",
    "- Sends one render request containing all clips in one timeline  \n",
    "- Returns render ID  \n",
    "\n",
    "Shotstack HTTP GET  \n",
    "- Polls render status until done  \n",
    "- Returns final video URL  \n",
    "\n",
    "This creates **one final MP4 link** that contains **multiple scenes in sequence**, matching the goal of one complete explainer video per judgment.\n",
    "\n",
    "c. Justification of design choices  \n",
    "\n",
    "This design is chosen because it solves the problem in the most scalable and automation-friendly way:  \n",
    "- Gemini is used because legal text is unstructured and requires intelligent summarization and restructuring.  \n",
    "- JSON output is essential because Make automation depends on reliable fields to map into downstream modules.  \n",
    "- Iterator + Aggregator is necessary because:  \n",
    "  - the system generates multiple scenes  \n",
    "  - Shotstack requires a single combined timeline for one final render  \n",
    "- Shotstack is used because it renders videos programmatically and returns a hosted output URL without manual editing.  \n",
    "\n",
    "This combination makes the system practical, repeatable, and suitable for real product deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4850174",
   "metadata": {},
   "source": [
    "4) Core Implementation  \n",
    "a. Model training / inference logic  \n",
    "\n",
    "No training was done because this project does not require training a custom ML model. The pipeline is **inference-only**, meaning:  \n",
    "- Gemini generates the structured script from extracted judgment text  \n",
    "- Shotstack renders the final video from structured clips  \n",
    "\n",
    "b. Prompt engineering (for LLM-based projects)  \n",
    "\n",
    "Prompt engineering was a core part of making the system stable. Gemini was instructed to output a strict JSON structure:  \n",
    "- must output a JSON object only  \n",
    "- must include scenes as an array  \n",
    "- each scene must contain:  \n",
    "  - scene_number  \n",
    "  - duration_seconds  \n",
    "  - narration  \n",
    "  - visual  \n",
    "\n",
    "This was necessary because earlier the pipeline failed when Gemini returned:  \n",
    "- extra text before/after JSON  \n",
    "- markdown formatting  \n",
    "- unexpected null values  \n",
    "- extra unsupported keys  \n",
    "\n",
    "To prevent failures, prompts were improved to ensure:  \n",
    "- plain JSON output only  \n",
    "- no markdown or backticks  \n",
    "- consistent scene structure for Make parsing and Shotstack rendering  \n",
    "\n",
    "c. Recommendation or prediction pipeline  \n",
    "\n",
    "Not applicable. This is not a recommendation system. It is a generation pipeline that converts:  \n",
    "**Judgment PDF → extracted text → structured scenes → video timeline → final video URL**\n",
    "\n",
    "d. Code must run top-to-bottom without errors  \n",
    "\n",
    "In the working version (multi-scene video generation), the pipeline ran end-to-end successfully:  \n",
    "- Gemini output → parsed correctly  \n",
    "- Iterator created multiple bundles  \n",
    "- Aggregator produced a complete clips array  \n",
    "- Shotstack POST request returned success (201 Created)  \n",
    "- Shotstack GET returned status done + final URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ae741",
   "metadata": {},
   "source": [
    "5) Evaluation & Analysis  \n",
    "a. Metrics used (quantitative or qualitative)  \n",
    "\n",
    "This system was evaluated using qualitative and functional metrics that directly reflect real user outcomes, such as:  \n",
    "- Functional success: Shotstack returns status done  \n",
    "- Final output URL generated successfully  \n",
    "- Scene correctness: video contains multiple scenes (not just one clip)  \n",
    "- Timing correctness: each scene’s duration matches duration_seconds  \n",
    "- Start times accumulate properly (0, 5, 15, 27…)  \n",
    "- Content quality: narration explains the judgment clearly in simple terms and captures facts, issues, arguments, and reasoning  \n",
    "\n",
    "b. Sample outputs / predictions  \n",
    "\n",
    "The pipeline produces:  \n",
    "- One final Shotstack render ID  \n",
    "- One final MP4 video URL (single link)  \n",
    "- A multi-scene text-based video where scenes appear in correct order  \n",
    "- A readable and consistent output style (white text on black background)  \n",
    "\n",
    "c. Performance analysis and limitations  \n",
    "\n",
    "Strengths  \n",
    "- Fully automated end-to-end system  \n",
    "- Works reliably without manual editing  \n",
    "- Multi-scene output is correct and structured  \n",
    "- Produces one combined final video link per judgment  \n",
    "\n",
    "Limitations (in this completed stage)  \n",
    "- Audio is not added yet  \n",
    "- Visuals are simple text-based (not animated graphics)  \n",
    "- Output quality depends on Gemini’s summarization consistency  \n",
    "- Video style is minimal (text scenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1717da3",
   "metadata": {},
   "source": [
    "6) Ethical Considerations & Responsible AI  \n",
    "a. Bias and fairness considerations  \n",
    "\n",
    "Legal summarization can unintentionally introduce bias by:  \n",
    "- emphasizing one party’s narrative more than the other  \n",
    "- omitting critical legal reasoning  \n",
    "- oversimplifying complex disputes  \n",
    "\n",
    "We reduce this risk by:  \n",
    "- keeping summaries neutral, factual, and educational  \n",
    "- avoiding emotional or accusatory language  \n",
    "- using neutral terms like “Party A” and “Party B” instead of assuming relationships  \n",
    "\n",
    "b. Dataset limitations  \n",
    "\n",
    "This project does not use a fixed dataset, and input judgments vary heavily. Limitations include:  \n",
    "- scanned PDFs may result in poor extraction quality  \n",
    "- formatting issues can reduce context clarity  \n",
    "- court orders may be incomplete or not structured uniformly  \n",
    "\n",
    "c. Responsible use of AI tools  \n",
    "\n",
    "The system is designed for educational understanding and simplified explanation. Users should treat the generated output as a learning tool and not as a replacement for reading the original judgment or professional legal interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec7fcb",
   "metadata": {},
   "source": [
    "7) Conclusion & Future Scope  \n",
    "a. Summary of results  \n",
    "\n",
    "We successfully built a working Make.com automation that:  \n",
    "✅ Takes legal judgment PDF text  \n",
    "✅ Uses Gemini to create a multi-scene script  \n",
    "✅ Iterates over scenes  \n",
    "✅ Aggregates them into one Shotstack timeline  \n",
    "✅ Generates one final video URL with multiple scenes  \n",
    "❌ Audio is not added yet in this version  \n",
    "\n",
    "b. Possible improvements and extensions  \n",
    "\n",
    "Future improvements after this stage include:  \n",
    "- Add voiceover tracks using ElevenLabs (TTS)  \n",
    "- Add multilingual audio generation (different languages)  \n",
    "- Add translation features so judgments in any language can be converted into explainer videos in the same language  \n",
    "- Add background visuals or images per scene  \n",
    "- Add subtitles and improved typography  \n",
    "- Add background music at low volume  \n",
    "- Add automatic polling loop for Shotstack GET (instead of manual rerun)  \n",
    "- Add fallback if Gemini JSON fails (repair step)  \n",
    "- Store final URL back to Bubble database for LawStory AI UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25bc6a",
   "metadata": {},
   "source": [
    "## Example: Structured JSON output used in the pipeline\n",
    "\n",
    "Below is a sample JSON format representing the multi-scene output that the Gemini module generates and Make.com parses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = {\n",
    "  \"title_frame\": {\n",
    "    \"court\": \"Example Court\",\n",
    "    \"case_title\": \"Party A v. Party B\",\n",
    "    \"year\": \"2025\",\n",
    "    \"citation\": \"Example Citation\",\n",
    "    \"coram\": \"Example Coram\"\n",
    "  },\n",
    "  \"scenes\": [\n",
    "    {\n",
    "      \"scene_number\": 1,\n",
    "      \"duration_seconds\": 10,\n",
    "      \"narration\": \"Scene 1 narration text...\",\n",
    "      \"visual\": \"White text on black background...\"\n",
    "    },\n",
    "    {\n",
    "      \"scene_number\": 2,\n",
    "      \"duration_seconds\": 15,\n",
    "      \"narration\": \"Scene 2 narration text...\",\n",
    "      \"visual\": \"White text on black background...\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "print(json.dumps(example_output, indent=2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
